{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : MiniNN Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 décembre 2016  \n",
    "Adapté du TP de Gaétan Marceau-Caron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Au cours du TP1, nous avons étudié le modèle _Softmax_ (aussi connu sous le nom de MaxEnt) pour traiter le problème de classification probabiliste.\n",
    "Le but était de présenter deux étapes importantes de l'entraînement: la {\\it forward propagation} et la mise à jour des paramètres.\n",
    "Le TP2 reprend le modèle Softmax dans un cadre plus général, celui des réseaux de neurones avec couches cachées.\n",
    "\n",
    "Dans ce cadre, on peut considérer le modèle Softmax comme un \"module\" qui prend en entrée des \"features\", e.g. les pixels d'une image, et qui donne en sortie une loi de probabilité sur les étiquettes.\n",
    "D'un point de vue computationnel, un réseau de neurones est composé de plusieurs modules, transformant simplement les features d'un espace à un autre en fonction des valeurs courantes des paramètres.\n",
    "Ainsi, le but de l'entraînement est d'apprendre les transformations pertinentes, i.e., en modifiant les paramètres, qui permettront de réaliser la tâche associée au module de sortie. \n",
    "En augmentant le nombre de modules (mais aussi de fonctions non-linéaires), on augmente ainsi la complexité du modèle.\n",
    "La thèse du {\\it Deep Learning} nous dit que les modules près des données d'entrée doivent apprennent des features de bas niveau, e.g., filtre de Gabor pour l'image, alors que les modules près de la sortie apprennent des features de haut niveau, e.g., la probabilité qu'il y ait un chat dans l'image.\n",
    "A priori, cette hiéarchie des features n'est pas imposée par le programmeur, mais apparaît naturellement lors de l'entraînement avec l'algorithme de backpropagation .\n",
    "\n",
    "Le but du TP2 est de programmer les trois étapes essentielles à l'entraînement d'un réseau de neurones: la forward propagation, la backpropagation et la mise à jour des paramètres.\n",
    "Ensuite, vous pouvez créer un test permettant de vérifier l'implémentation: le test des différences finies.\n",
    "Finalement, vous pourrez comparer les performances de votre réseau de neurones avec celles de votre modèle Softmax de la semaine dernière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs : \n",
    "    1. Télécharger le TP2\n",
    "    2. Implementer la fonction forward (nn ops.py:72)\n",
    "    3. Implementer la fonction sigmoid et sa d ́eriv ́ee (nn ops.py:149)\n",
    "    4. Implementer la fonction backward (nn ops.py:93)\n",
    "    5. Implementer la fonction update (nn ops.py:123)\n",
    "    6. Entrainer miniNN et obtenir une accuracy meilleur que le modèle softmax (de 0.92)\n",
    "\n",
    "\n",
    "Et optionellemnt : \n",
    "    7. Implementer le test des diff ́erences finies (fd test.py)\n",
    "    \n",
    "La différence finie est une approximation de la dérivée partielle:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l(w)}{\\partial w_{i}} \\approx \\frac{l(w + \\epsilon e_{i}) - l(w - \\epsilon e_{i})}{2 \\epsilon}\n",
    "\\end{equation}\n",
    "où $l$ est une fonction à plusieurs variables avec ses dérivées partielles définies, $w$ est un vecteur, $w_{i}$ est sa $i$ème composante, $\\epsilon$ est la longeur du pas et $e_{i}$ est le $i$ème vecteur de la base canonique de l'espace euclidien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrable\n",
    "\n",
    "- __Date du livrable__: Le 16 décembre 2015  \n",
    "- __Format du livrable__ un fichier compressé nommé \"DL\\_tp2\\_prénom\\_nom.zip\" contenant le code et le résumé   \n",
    "- __Dépôt__ à l'adresse _{thomas.schmitt@inria.fr}_ avec comme objet du message \"DL\\_tp2\\_prénom\\_nom\".  \n",
    "- __Description__  \n",
    "Le livrable associé au TP2 doit contenir le code de MiniNN complété et accompagné d'un résumé de une à deux pages.\n",
    "Le code doit s'exécuter avec la commande _python miniNN.py_ et afficher l'évolution de l'apprentissage (sortie par défaut du programme).  \n",
    "( Ou bien être sous la forme d'un notebook. )\n",
    "\n",
    "Le résumé doit être succinct et se focaliser uniquement sur les points essentiels reliés à l'entraînement des réseaux de neurones.\n",
    "Ce document doit décrire les difficultés que vous avez rencontrées et, dans le cas échéant, les solutions utilisées pour les résoudre.\n",
    "Vous pouvez aussi y décrire vos questions ouvertes et proposer une expérience sur MNIST afin d'y répondre.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zone de Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and minibatch (as in miniNN.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pour ne pas reload le kernel quand un fichier .py change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, math, time, sys\n",
    "import dataset_loader\n",
    "from nn_ops import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve the arguments \n",
    "args = parseArgs_ipython(arch = [100], act_func = \"sigmoid\", batch_size = 500, eta = .01, n_epoch = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of the experiment\n",
      "----------\n",
      "Learning algorithm: Bprop\n",
      "Initial step-size: 0.01\n",
      "Network Architecture: [784 100  10]\n",
      "Number of parameters: -1\n",
      "Minibatch size: 500\n",
      "Activation: sigmoid\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types dtype('<U21') dtype('<U21') dtype('<U21')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-137728812707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mprintDescription\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bprop\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_arch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_func_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/thom/Documents/these/enseignements/M2 IAC/tp-nnet-aic/tp2/code/nn_ops_tom.py\u001b[0m in \u001b[0;36mprintDescription\u001b[1;34m(algo_name, eta, nn_arch, act_func_name, minibatch_size, regularizer, drop_func, nb_param, lamda, sparsity)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Activation: \"\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[0mact_func_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Regularization: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" with \"\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlamda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdrop_func\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dropout: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdrop_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types dtype('<U21') dtype('<U21') dtype('<U21')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fix the seed for the random generator\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "#############################\n",
    "### Dataset Handling\n",
    "#############################\n",
    "\n",
    "### Load the dataset\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()\n",
    "\n",
    "### Define the dataset variables\n",
    "n_training = train_set[0].shape[0]\n",
    "n_feature = train_set[0].shape[1]\n",
    "n_label = np.max(train_set[1])+1\n",
    "\n",
    "#############################\n",
    "### Neural Network parameters\n",
    "#############################\n",
    "\n",
    "### Activation function\n",
    "act_func_name = args.act_func\n",
    "\n",
    "### Network Architecture\n",
    "nn_arch = np.array([n_feature] + args.arch + [n_label])\n",
    "\n",
    "### Create the neural network\n",
    "W,B,act_func,nb_params = initNetwork(nn_arch,act_func_name)\n",
    "\n",
    "#############################\n",
    "### Optimization parameters\n",
    "#############################\n",
    "eta = args.eta\n",
    "batch_size = args.batch_size\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = args.n_epoch \n",
    "\n",
    "#############################\n",
    "### Auxiliary variables\n",
    "#############################\n",
    "cumul_time = 0.\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "printDescription(\"Bprop\", eta, nn_arch, act_func_name, batch_size, nb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Mini-batch creation\n",
    "j = 0\n",
    "batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "X_bacth = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appel de chaque fonction, avec les valeurs calculé avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Forward propagation\n",
    "Y,Yp = forward(act_func, W, B, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert Y[0].shape == (n_feature, batch_size)\n",
    "assert Y[-1].shape == (10, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert Yp[0].shape == (nn_arch[1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Compute the softmax\n",
    "out = softmax(Y[-1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert out.shape == (nn_arch[-1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Compute the gradient at the top layer\n",
    "derror = out-one_hot_batch\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert derror.shape == (nn_arch[-1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Backpropagation\n",
    "gradB = backward(derror, W, Yp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(gradB) == len(nn_arch)-1\n",
    "for gradw,dim in zip(gradB,nn_arch[1:]):\n",
    "    gradw.shape = (dim,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Update the parameters\n",
    "new_W, new_B = update(eta, batch_size, W, B, gradB, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Training accuracy\n",
    "train_loss, train_accuracy = computeLoss(W, B, train_set[0], train_set[1], act_func) \n",
    "\n",
    "### Valid accuracy\n",
    "valid_loss, valid_accuracy = computeLoss(W, B, valid_set[0], valid_set[1], act_func) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta = 1\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Y,Yp = forward(act_func, W, B, batch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Y[-1])\n",
    "        \n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = out-one_hot_batch\n",
    "\n",
    "        ### Backpropagation\n",
    "        gradB = backward(derror, W, Yp)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W, B = update(eta, batch_size, W, B, gradB, Y)\n",
    "\n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "\n",
    "    ### Training accuracy\n",
    "    train_loss, train_accuracy = computeLoss(W, B, train_set[0], train_set[1], act_func) \n",
    "\n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_accuracy = computeLoss(W, B, valid_set[0], valid_set[1], act_func) \n",
    "\n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_accuracy) + \" \" + str(valid_loss) + \" \" + str(valid_accuracy) + \" \" + str(eta)\n",
    "\n",
    "    print(result_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(0,label='train_acc')\n",
    "plt.plot(0,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
